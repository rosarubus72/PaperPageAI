import os
import re
import json
import torch
import numpy as np
from pathlib import Path
from jinja2 import Environment, FileSystemLoader
from sklearn.metrics.pairwise import cosine_similarity
from transformers import (
    AutoTokenizer,
    AutoModel,
    AutoModelForCausalLM,
)
import shutil
import pandas as pd
from llms.llm2 import LLM2
from bs4 import BeautifulSoup
from bs4 import Comment
# ==========================================================
# ğŸ”¹ ç®€åŒ–çš„è¡¨æ ¼è§£æå™¨ - ä¸“æ³¨äºVLMç”Ÿæˆ
# ==========================================================
import html
import re

class PlannerAgent:
    def __init__(self, llm_model):
        self.llm_model = llm_model
    
    def plan_homepage_sections(self, paper_content, modules_content):
        """æ ¹æ®è®ºæ–‡å†…å®¹è§„åˆ’ä¸»é¡µåº”è¯¥å±•ç¤ºå“ªäº›éƒ¨åˆ†ï¼Œç¡®ä¿è¿è´¯æ€§"""
        # ç›´æ¥è¿”å›æ¨¡æ¿ä¸­å›ºå®šçš„éƒ¨åˆ†
        fixed_sections = ["abstract", "motivation", "innovation", "methodology", "experiments"]
        print(f"âœ… PlannerAgentè¿”å›å›ºå®šéƒ¨åˆ†: {fixed_sections}")
        return fixed_sections
    
    def _parse_plan_response(self, resp):
        """è§£æè§„åˆ’å“åº” - å¤‡ç”¨æ–¹æ³•"""
        # è¿”å›å›ºå®šéƒ¨åˆ†
        return ["abstract", "motivation", "innovation", "methodology", "experiments"]

# ==========================================================
# ğŸ”¹ æ–‡æœ¬æ ¼å¼åŒ–å·¥å…·
# ==========================================================
class TextFormatter:
    @staticmethod
    def clean_caption(caption):
        """æ¸…ç†å›¾è¡¨æ ‡é¢˜ï¼Œç§»é™¤Figure Xã€Table Xç­‰å‰ç¼€"""
        if not caption:
            return ""
        
        # å¸¸è§çš„å›¾è¡¨å‰ç¼€æ¨¡å¼
        patterns = [
            r'^Figure\s*\d+[\.:]\s*',      # Figure 1: æˆ– Figure 1.
            r'^Fig\.\s*\d+[\.:]\s*',       # Fig. 1: æˆ– Fig. 1.
            r'^Table\s*\d+[\.:]\s*',       # Table 1: æˆ– Table 1.
            r'^Tab\.\s*\d+[\.:]\s*',       # Tab. 1: æˆ– Tab. 1.
            r'^FIG\.\s*\d+[\.:]\s*',       # FIG. 1: æˆ– FIG. 1.
            r'^TABLE\s*\d+[\.:]\s*',       # TABLE 1: æˆ– TABLE 1.
            r'^Fig\s*\d+[\.:]\s*',         # Fig 1: æˆ– Fig 1.
            r'^Tab\s*\d+[\.:]\s*',         # Tab 1: æˆ– Tab 1.
        ]
        
        cleaned_caption = caption.strip()
        for pattern in patterns:
            # å°è¯•åŒ¹é…å¹¶ç§»é™¤å‰ç¼€
            cleaned_caption = re.sub(pattern, '', cleaned_caption, flags=re.IGNORECASE)
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›åŸå§‹æ ‡é¢˜
        if not cleaned_caption.strip():
            return caption.strip()
        
        # ç¡®ä¿é¦–å­—æ¯å¤§å†™
        cleaned_caption = cleaned_caption.strip()
        if cleaned_caption and cleaned_caption[0].islower():
            cleaned_caption = cleaned_caption[0].upper() + cleaned_caption[1:]
        
        return cleaned_caption
    
    @staticmethod
    def format_text(text):
        """å¤„ç†åŠ ç²—ã€åˆ—è¡¨ã€é‡ç‚¹é¢œè‰²ç­‰æ ¼å¼è½¬æ¢"""
        if not text:
            return ""
            
        # 1. è½¬æ¢åŠ ç²—ä¸º<strong>æ ‡ç­¾
        text = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', text)
        
        # 2. è½¬æ¢æ•°å­—åˆ—è¡¨ï¼ˆ1. 2. 3.ï¼‰ä¸ºæœ‰åºåˆ—è¡¨
        lines = text.split('\n')
        in_list = False
        formatted_lines = []
        for line in lines:
            line = line.strip()
            if re.match(r'^\d+\.', line):
                if not in_list:
                    formatted_lines.append('<ol class="list-decimal pl-6 space-y-2">')
                    in_list = True
                # æå–åˆ—è¡¨å†…å®¹å¹¶ä¿ç•™æ ¼å¼
                content = re.sub(r'^\d+\.\s*', '', line)
                formatted_lines.append(f'  <li>{content}</li>')
            else:
                if in_list:
                    formatted_lines.append('</ol>')
                    in_list = False
                formatted_lines.append(line)
        if in_list:
            formatted_lines.append('</ol>')
        text = '\n'.join(formatted_lines)
        
        # 3. ä¸ºç‰¹å®šå…³é”®è¯æ·»åŠ é¢œè‰²
        keywords = [
            r'PosterAgent', r'Qwen', r'GPT-4o',  # æ¨¡å‹å
            r'Visual Quality', r'Textual Coherence', r'PaperQuiz'  # æŒ‡æ ‡å
        ]
        for kw in keywords:
            text = re.sub(
                fr'({kw})',
                r'<span class="text-primary font-semibold">\1</span>',
                text,
                flags=re.IGNORECASE
            )
        
        return text

# ==========================================================
# ğŸ”¹ BGE æ¨¡å‹
# ==========================================================
class BGEEmbedder:
    def __init__(self, model_path):
        device = torch.device("cuda:2")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(
            model_path, dtype=torch.float16, device_map={"": device}
        )
        self.model.eval()

    def encode(self, texts, batch_size=8):
        if isinstance(texts, str):
            texts = [texts]
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            inputs = self.tokenizer(
                batch, padding=True, truncation=True,
                return_tensors="pt", max_length=512
            ).to(self.model.device)
            with torch.no_grad():
                output = self.model(**inputs)
                batch_emb = output.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.append(batch_emb)
        return np.vstack(embeddings)

    def similarity(self, query, candidates, top_k=5):
        q_emb = self.encode([query])
        c_emb = self.encode([c["text"] for c in candidates])
        sims = cosine_similarity(q_emb, c_emb).flatten()
        top_indices = sims.argsort()[::-1][:top_k]
        return [(candidates[i], float(sims[i])) for i in top_indices]

# ==========================================================
# ğŸ”¹ Qwen å†³ç­–å™¨
# ==========================================================
class QwenDecider:
    def __init__(self, llm_model):
        self.llm_model = llm_model
        print("âœ… Qwen2.5-7B-Instruct loaded via LLM2.")
    
    def _get_module_specific_guidance(self, module_name):
        """ä¸ºç‰¹å®šæ¨¡å—æä¾›è§†è§‰é€‰æ‹©æŒ‡å¯¼"""
        guidance_map = {
            "introduction": """
        - PRIORITY: Conceptual diagrams, problem illustrations, motivation figures
        - ACCEPTABLE: High-level architecture overviews (if essential for context)
        - AVOID: Detailed experimental results, technical parameter tables
        - RESERVE: All quantitative results for experiments sections""",

            "background": """
        - PRIORITY: Comparative analysis with prior work, domain context figures
        - ACCEPTABLE: Foundational concept illustrations
        - AVOID: Novel methodology diagrams, experimental results
        - RESERVE: Your technical innovations for methodology section""",

            "methodology": """
        - PRIORITY: Architecture diagrams, process flows, algorithm visualizations
        - ACCEPTABLE: Parameter tables, component specifications
        - AVOID: Experimental results, performance comparisons
        - RESERVE: All results for dedicated experiments sections""",

            "method": """
        - PRIORITY: Architecture diagrams, process flows, algorithm visualizations
        - ACCEPTABLE: Parameter tables, component specifications
        - AVOID: Experimental results, performance comparisons
        - RESERVE: All results for dedicated experiments sections""",

            "experiments": """
        - PRIORITY: Results tables, performance charts, ablation studies
        - ACCEPTABLE: Comparison figures, statistical analyses
        - AVOID: Conceptual diagrams, architecture overviews
        - USE NOW: This is the primary section for experimental visuals""",

            "results": """
        - PRIORITY: Quantitative results, evaluation metrics, benchmark comparisons
        - ACCEPTABLE: Visualization of findings, statistical significance
        - AVOID: Methodological diagrams, conceptual illustrations
        - USE NOW: Critical results should be displayed here""",

            "innovation": """
        - PRIORITY: Novel framework diagrams, comparison with existing methods
        - ACCEPTABLE: Technical novelty illustrations
        - AVOID: Detailed experimental results
        - RESERVE: Results for experiments sections""",

            "motivation": """
        - PRIORITY: Problem illustrations, motivation diagrams
        - ACCEPTABLE: High-level overviews
        - AVOID: Technical details, experimental results
        - RESERVE: Technical diagrams for methodology section"""
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„æŒ‡å¯¼
        module_lower = module_name.lower()
        for key, guidance in guidance_map.items():
            if key in module_lower:
                return guidance
        
        # é»˜è®¤æŒ‡å¯¼
        return """
        - Assess if this module is primarily: conceptual, methodological, experimental, or summary-oriented
        - Reserve experimental results for experiments/results sections
        - Save technical diagrams for methodology/approach sections  
        - Use conceptual illustrations for introduction/motivation
        - When uncertain, err toward preserving visuals for more appropriate sections"""
    
    def decide_visuals(self, module_name, summary_text, candidates, used_visuals, max_new_tokens=200):
        # è¿‡æ»¤å·²ä½¿ç”¨çš„è§†è§‰å…ƒç´ 
        available_candidates = [
            (v, score) for v, score in candidates 
            if (v.get("image_path") or v.get("table_path")) not in used_visuals
        ]
        if not available_candidates:
            return []

        candidate_text = ""
        for i, (v, score) in enumerate(available_candidates, 1):
            vtype = "Table" if "table_path" in v else "Figure"
            cap = v.get("caption", "")
            extra = v.get("table_text", "")[:300] if "table_path" in v else ""
            candidate_text += f"\n[{i}] ({vtype}, sim={score:.3f}) {cap}\n{extra}\n"

        prompt = f"""
You are a strategic visual resource allocator for a research paper webpage design.

CRITICAL ALLOCATION RULES:
1. STRICT ONE-TIME USE: Each visual can only be used ONCE in the entire project
2. SEQUENTIAL PRESERVATION: Never use visuals that appear later in the paper for earlier sections
3. TYPE-TO-SECTION MATCHING: Allocate visual types to their most appropriate sections

Current Module: {module_name}
Module Summary: {summary_text}

Available Candidate Visuals (NOT used in other modules):
{candidate_text}

**VISUAL TYPE TO SECTION MAPPING - STRICT GUIDELINES:**

**EXPERIMENTAL VISUALS â†’ Reserve for Experiments/Results Sections:**
- Results tables, performance comparisons, ablation studies
- Quantitative evaluation charts, statistical analyses
- Benchmark comparison figures, accuracy/loss curves
- DO NOT use these in Introduction/Motivation sections

**METHODOLOGY VISUALS â†’ Reserve for Methodology Sections:**
- Model architecture diagrams, system overviews
- Process flowcharts, algorithm pseudocode illustrations
- Technical component diagrams, framework schematics
- Parameter tables, configuration specifications

**CONCEPTUAL VISUALS â†’ Use in Introduction/Motivation/Innovation Sections:**
- Problem illustrations, motivation diagrams
- Conceptual frameworks, high-level overviews
- Comparative analysis with prior work
- Domain-specific illustrative examples

**STRATEGIC SELECTION CRITERIA:**

1. **RELEVANCE ASSESSMENT:**
   - Does this visual directly support the core message of {module_name}?
   - Is there a stronger alignment with another module based on visual content?

2. **TYPE-SECTION ALIGNMENT:**
   - Experimental results â†’ Experiments sections ONLY
   - Technical diagrams â†’ Methodology sections ONLY  
   - Conceptual figures â†’ Introduction/Motivation/Innovation sections ONLY

3. **IMPACT PRESERVATION:**
   - Save high-impact experimental visuals for results demonstration
   - Reserve technical architecture diagrams for methodology explanation
   - Keep conceptual illustrations for problem motivation/innovation

4. **CONSERVATIVE ALLOCATION:**
   - Select 0-2 visuals ONLY if they provide exceptional value
   - When in doubt, preserve the visual for potentially better-suited modules
   - Prioritize cross-module resource optimization over individual module completeness

**MODULE-SPECIFIC GUIDANCE FOR {module_name}:**

{self._get_module_specific_guidance(module_name)}

Return ONLY a JSON list of indices, e.g., [1,3] or [] if none are highly suitable.
Be extremely selective to preserve the most appropriate visuals for their ideal sections.
"""

        resp = self.llm_model.generate(query=prompt)

        try:
            matched = json.loads(re.search(r"\[.*?\]", resp, re.S).group())
        except Exception:
            matched = []
        return [available_candidates[i - 1][0] for i in matched if 1 <= i <= len(available_candidates)]



class SectionContentAgent:
    """ç« èŠ‚å†…å®¹ç”Ÿæˆä»£ç†ï¼Œä½¿ç”¨LLM2è¿›è¡Œåˆ†å±‚æ£€ç´¢å’Œå†…å®¹ç”Ÿæˆ"""
    
    def __init__(self, llm_model):
        self.llm_model = llm_model
        
    def retrieve_relevant_titles(self, section_name, paper_sections, top_k=5):
        """æ£€ç´¢ä¸section_nameç›¸å…³çš„ç« èŠ‚æ ‡é¢˜ - å¢å¼ºç‰ˆæœ¬"""
        
        # åˆ›å»ºsection_nameçš„åŒä¹‰è¯æ˜ å°„
        synonym_map = {
            "experiments": ["experiments", "experimental", "evaluation", "results", 
                        "performance", "benchmark", "analysis", "validation"],
            "methodology": ["methodology", "method", "approach", "technical", 
                        "framework", "architecture", "system"],
            "innovation": ["innovation", "contribution", "novelty", "technical_contribution"],
            "motivation": ["motivation", "introduction", "background", "problem"],
            "abstract": ["abstract", "summary", "overview"],
        }
        
        # è·å–ç›®æ ‡éƒ¨åˆ†çš„æ‰€æœ‰å¯èƒ½å…³é”®è¯
        target_keywords = synonym_map.get(section_name.lower(), [section_name.lower()])
        
        # å…ˆå°è¯•ä½¿ç”¨LLMæ£€ç´¢
        prompt = f"""Based on the given section name and paper structure, retrieve the most relevant section titles.
        Section name: {section_name}
        Possible keywords: {', '.join(target_keywords)}
        
        List of paper sections:
        {self._format_sections_list(paper_sections)}
        
        Please analyze and select the top {top_k} most relevant section titles from the list.
        Return only the list of section titles in order, with no explanations.
        Relevant section titles:"""
        
        try:
            response = self.llm_model.generate(query=prompt)
            titles = self._parse_titles_from_response(response, paper_sections)
                        
            return titles[:top_k]
        except Exception as e:
            print(f"âŒ æ£€ç´¢ç›¸å…³æ ‡é¢˜å¤±è´¥: {e}")
            return self._keyword_based_retrieval(target_keywords, paper_sections, top_k)
    
    def generate_section_content(self, section_name, relevant_sections, previously_generated_content=None):
        """åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³ç« èŠ‚ç”Ÿæˆå†…å®¹ï¼Œè€ƒè™‘å·²æœ‰å†…å®¹é¿å…é‡å¤"""
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé¿å…é‡å¤
        context_info = ""
        if previously_generated_content:
            context_info = f"""
**ALREADY COVERED IN OTHER SECTIONS (DO NOT REPEAT):**
{self._summarize_previous_content(previously_generated_content)}

**CRITICAL: Ensure this section introduces NEW information not covered above.**
"""
        
        prompt = f"""
You are an academic paper homepage assistant. Generate exceptionally clear, concise, and coherent content for the "{section_name}" section that flows naturally within the overall research narrative.

**COHERENCE AND UNIQUENESS REQUIREMENTS:**
- Create content that logically connects to the broader research story
- Introduce NEW information not covered in other sections
- Build upon concepts introduced in previous sections naturally
- Avoid repeating facts, examples, or explanations from other sections
- Ensure smooth conceptual flow between ideas

{context_info}

**Source Content:**
{self._format_relevant_sections(relevant_sections)}

**Narrative Flow Guidelines:**
â€¢ Start with content that naturally follows from previous sections
â€¢ Introduce concepts in logical sequence (generalâ†’specific, problemâ†’solution)
â€¢ Use transitional language to connect ideas within the section
â€¢ Each paragraph should advance the section's unique contribution
â€¢ Maintain consistent terminology and conceptual framework

**Content Generation Strategy:**
â€¢ Extract the MOST ESSENTIAL information unique to this section
â€¢ Focus on this section's specific role in the research narrative
â€¢ Use connecting phrases to show relationship to broader context
â€¢ Apply <strong> only to 2-3 most important NEW concepts
â€¢ Eliminate any information redundant with other sections
â€¢ Ensure each sentence adds new value to the reader

**Section-Specific Focus:**
{self._get_section_focus_guidance(section_name)}

**Output Specifications:**
- Begin with content that naturally continues the research story
- No section titles, headings, or repetitive introductory phrases
- Ensure conceptual continuity with overall paper narrative
- Maintain consistent academic tone and terminology
- Keep length appropriate (typically 2-4 well-connected paragraphs)
- Verify NO overlap with content from other sections

Generate the pure content body for the {section_name} section:
"""
        
        try:
            response = self.llm_model.generate(query=prompt)
            return self._clean_generated_content(response)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return self._fallback_content_generation(section_name, relevant_sections)
    
    def _summarize_previous_content(self, previous_content):
        """æ€»ç»“å·²ç”Ÿæˆçš„å†…å®¹ï¼Œå¸®åŠ©é¿å…é‡å¤"""
        summary = []
        for section_name, content in previous_content.items():
            # æå–å…³é”®å¥å­ï¼ˆå‰2å¥ï¼‰
            sentences = re.split(r'[.!?]', content)
            key_sentences = [s.strip() for s in sentences[:2] if s.strip()]
            if key_sentences:
                summary.append(f"- {section_name}: {' '.join(key_sentences)}")
        
        return "\n".join(summary) if summary else "No previous content generated yet."

    def _get_section_focus_guidance(self, section_name):
        """ä¸ºä¸åŒç« èŠ‚æä¾›å…·ä½“çš„å†…å®¹èšç„¦æŒ‡å¯¼"""
        focus_guidance = {
            "abstract": "Focus on overall contribution and significance - avoid detailed methodology or results",
            "motivation": "Emphasize problem importance and research gap - don't repeat introduction content",
            "innovation": "Highlight technical novelty and key innovations - avoid repeating methodology details",
            "methodology": "Explain core approach and technical framework - save implementation details for experiments",
            "experiments": "Focus on experimental setup and key findings - don't re-explain methodology"
        }
        
        section_lower = section_name.lower()
        for key, guidance in focus_guidance.items():
            if key in section_lower:
                return guidance
        
        return "Focus on this section's unique contribution to the overall research narrative."
    
    def _format_sections_list(self, paper_sections):
        """æ ¼å¼åŒ–ç« èŠ‚åˆ—è¡¨ç”¨äºæç¤º"""
        sections_list = []
        for i, section in enumerate(paper_sections):
            title = section.get("title", "").strip()
            if title:
                sections_list.append(f"{i+1}.\n")
        return "\n".join(sections_list)
    
    def _parse_titles_from_response(self, response, paper_sections):
        """ä»æ¨¡å‹å“åº”ä¸­è§£æç« èŠ‚æ ‡é¢˜"""
        titles = []
        
        # å°è¯•å¤šç§è§£ææ–¹å¼
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            # åŒ¹é…ç¼–å·æ ¼å¼: "1. æ ‡é¢˜" æˆ– "- æ ‡é¢˜"
            match = re.match(r'^(\d+\.\s*|-\s*)(.+)', line)
            if match:
                potential_title = match.group(2).strip()
                # åœ¨è®ºæ–‡ç« èŠ‚ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ ‡é¢˜
                for section in paper_sections:
                    section_title = section.get("title", "").strip()
                    if section_title and self._titles_match(section_title, potential_title):
                        titles.append(section_title)
                        break
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if not titles:
            titles = self._fallback_title_matching(response, paper_sections)
            
        return list(set(titles))  # å»é‡
    
    def _titles_match(self, actual_title, extracted_title):
        """åˆ¤æ–­ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦åŒ¹é…"""
        actual_clean = re.sub(r'[^\w\s]', '', actual_title.lower())
        extracted_clean = re.sub(r'[^\w\s]', '', extracted_title.lower())
        
        # å®Œå…¨åŒ¹é…æˆ–åŒ…å«å…³ç³»
        return (actual_clean == extracted_clean or 
                actual_clean in extracted_clean or 
                extracted_clean in actual_clean)
    
    def _fallback_title_matching(self, response, paper_sections):
        """å¤‡ç”¨æ ‡é¢˜åŒ¹é…æ–¹æ³•"""
        titles = []
        response_lower = response.lower()
        
        for section in paper_sections:
            section_title = section.get("title", "").strip()
            if section_title:
                # ç®€å•çš„å…³é”®è¯åŒ¹é…
                title_keywords = set(re.findall(r'\b\w+\b', section_title.lower()))
                response_keywords = set(re.findall(r'\b\w+\b', response_lower))
                
                common_keywords = title_keywords.intersection(response_keywords)
                if len(common_keywords) >= 2:  # è‡³å°‘æœ‰2ä¸ªå…±åŒå…³é”®è¯
                    titles.append(section_title)
        
        return titles[:5]  # æœ€å¤šè¿”å›5ä¸ª
    
    def _format_relevant_sections(self, relevant_sections):
        """æ ¼å¼åŒ–ç›¸å…³ç« èŠ‚å†…å®¹"""
        formatted = []
        for section in relevant_sections:
            title = section.get("title", "Untitled")
            content = section.get("content", "").strip()
            if content:
                formatted.append(f"ã€{title}ã€‘\n{content}\n")
        return "\n".join(formatted)
    
    def _clean_generated_content(self, content):
        """æ¸…ç†ç”Ÿæˆçš„å†…å®¹"""
        # ç§»é™¤å¯èƒ½çš„æç¤ºè¯æ®‹ç•™
        content = re.sub(r'^(è¯·ç”Ÿæˆ|ç”Ÿæˆå†…å®¹|å†…å®¹:|#+)\s*', '', content, flags=re.IGNORECASE)
        content = content.strip()
        
        # ç¡®ä¿ä»¥å®Œæ•´çš„å¥å­ç»“æŸ
        if content and not content.endswith(('.', 'ã€‚')):
            content += '.'
            
        return content
    
    def _keyword_based_retrieval(self, target_keywords, paper_sections, top_k):
        """åŸºäºå…³é”®è¯çš„æ£€ç´¢æ–¹æ³•"""
        relevant_titles = []
        
        for section in paper_sections:
            title = section.get("title", "").lower()
            for keyword in target_keywords:
                if keyword in title:
                    relevant_titles.append(section.get("title", ""))
                    break
        
        return relevant_titles[:top_k]
    
    def _fallback_content_generation(self, section_name, relevant_sections):
        """å¤‡ç”¨å†…å®¹ç”Ÿæˆæ–¹æ³•"""
        if not relevant_sections:
            return f"Content for {section_name} not available."
        
        # ç®€å•æ‹¼æ¥ç›¸å…³å†…å®¹
        contents = []
        for section in relevant_sections:
            content = section.get("content", "").strip()
            if content:
                contents.append(content)
        
        if contents:
            # å–ç¬¬ä¸€ä¸ªç›¸å…³å†…å®¹ä½œä¸ºä¸»è¦å†…å®¹
            main_content = contents[0]
            # ç®€å•æˆªæ–­ä»¥é¿å…è¿‡é•¿
            if len(main_content) > 500:
                sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', main_content)
                summary = []
                total_length = 0
                for sentence in sentences:
                    if sentence.strip():
                        summary.append(sentence.strip())
                        total_length += len(sentence)
                        if total_length > 300:
                            break
                return '. '.join(summary) + '.'
            return main_content
        else:
            return f"Content for {section_name} not available."

# ==========================================================
# ğŸ”¹ ä¸»ç±»ï¼šç®€åŒ–ç‰ˆæœ¬
# ==========================================================
class PaperHomepageGenerator:
    def __init__(self, content_json_path, modules_json_path, template_path,
                 qwen_model, bge_path, images_json_path=None, tables_json_path=None, csv_path=None):
        self.content_json_path = content_json_path
        self.modules_json_path = modules_json_path
        self.images_json_path = images_json_path
        self.tables_json_path = tables_json_path
        self.template_path = template_path
        self.qwen_model = qwen_model  # ç°åœ¨ç›´æ¥ä¼ å…¥LLM2å®ä¾‹
        self.bge_path = bge_path
        self.csv_path = csv_path

        self.paper_content = self._load_json(self.content_json_path)
        self.modules_content = self._load_json(self.modules_json_path)
        self.images_data = self._load_json(self.images_json_path) or {}
        self.tables_data = self._load_json(self.tables_json_path) or {}
        self.link_data = self._load_csv_links()

        self.qwen = QwenDecider(self.qwen_model)  # ä¼ å…¥LLM2å®ä¾‹
        self.bge = BGEEmbedder(self.bge_path)
        
        # æ–°å¢ä»£ç†
        self.planner_agent = PlannerAgent(self.qwen_model)
        self.section_agent = SectionContentAgent(self.qwen_model)
        
        self.used_visuals = set()
        self.formatter = TextFormatter()
        self.table_counter = 0
        
        self.output_assets_dir = None
        self.assets_mapping = {}
        self.planned_sections = []  # å­˜å‚¨è§„åˆ’çš„éƒ¨åˆ†

    def _load_csv_links(self):
        """ä»CSVæ–‡ä»¶åŠ è½½è®ºæ–‡é“¾æ¥ä¿¡æ¯"""
        if not self.csv_path or not os.path.exists(self.csv_path):
            print(f"âš ï¸ CSVæ–‡ä»¶ä¸å­˜åœ¨: {self.csv_path}")
            return {}
        
        try:
            df = pd.read_csv(self.csv_path)
            link_dict = {}
            
            for _, row in df.iterrows():
                title = row.get('title', '').strip()
                if title:
                    link_dict[title] = {
                        'paper_url': row.get('paper_url', '#'),
                        'homepage': row.get('homepage', '#')
                    }
            
            print(f"âœ… ä»CSVåŠ è½½äº† {len(link_dict)} ç¯‡è®ºæ–‡çš„é“¾æ¥ä¿¡æ¯")
            return link_dict
            
        except Exception as e:
            print(f"âŒ åŠ è½½CSVé“¾æ¥å¤±è´¥: {e}")
            return {}

    def _load_json(self, path):
        if not path or not os.path.exists(path):
            return {}
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _setup_output_directory(self, output_path):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        output_dir = Path(output_path).parent
        html_filename = Path(output_path).name
        
        if '.' in html_filename:
            folder_name = html_filename.rsplit('.', 1)[0]
            output_folder = output_dir / folder_name
        else:
            output_folder = output_dir / html_filename
            
        output_folder.mkdir(parents=True, exist_ok=True)
        self.output_assets_dir = output_folder / "assets"
        self.output_assets_dir.mkdir(exist_ok=True)
        
        return output_folder / "index.html"

    def _copy_asset(self, original_path):
        """å¤åˆ¶èµ„æºæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•å¹¶è¿”å›ç›¸å¯¹è·¯å¾„"""
        if not original_path or not os.path.exists(original_path):
            return original_path
            
        if original_path in self.assets_mapping:
            return self.assets_mapping[original_path]
        
        original_file = Path(original_path)
        new_filename = f"asset_{len(self.assets_mapping)}_{original_file.name}"
        relative_path = f"assets/{new_filename}"
        
        destination = self.output_assets_dir / new_filename
        try:
            shutil.copy2(original_path, destination)
            self.assets_mapping[original_path] = relative_path
            print(f"âœ… å¤åˆ¶èµ„æº: {original_path} -> {relative_path}")
            return relative_path
        except Exception as e:
            print(f"âŒ å¤åˆ¶èµ„æºå¤±è´¥: {original_path}, é”™è¯¯: {e}")
            return original_path

    def _extract_basic_paper_info(self):
        """æå–è®ºæ–‡åŸºæœ¬ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€é“¾æ¥ï¼‰"""
        # æå–æ ‡é¢˜
        title = self.paper_content.get("sections", [{}])[0].get("title", "Untitled Paper")
        
        # æå–ä½œè€…ä¿¡æ¯
        first_content = self.paper_content.get("sections", [{}])[0].get("content", "")
        authors, affiliations, project_link = self._extract_authors_from_content(first_content)
        authors = authors or ["Anonymous"]
        
        # ä»CSVè·å–é“¾æ¥
        csv_links = self._get_links_for_paper(title)
        paper_url = csv_links.get('paper_url', '#')
        homepage_url = csv_links.get('homepage', '#')
        
        # å°è¯•æå–å‘è¡¨ä¿¡æ¯
        publication_info = self._extract_publication_info()
        
        return {
            "title": title,
            "authors": ', '.join(authors),
            "affiliations": ', '.join(affiliations),
            "publication_info": publication_info,
            "year": "2025",  # é»˜è®¤å¹´ä»½
            "links": {
                "paper": paper_url,
                "project_page": homepage_url
            }
        }

    def _extract_publication_info(self):
        """å°è¯•ä»è®ºæ–‡å†…å®¹ä¸­æå–å‘è¡¨ä¿¡æ¯"""
        # æ£€æŸ¥å‰å‡ ä¸ªç« èŠ‚
        for i, section in enumerate(self.paper_content.get("sections", [])[:5]):
            content = section.get("content", "")
            # æŸ¥æ‰¾å¯èƒ½çš„ä¼šè®®/æœŸåˆŠä¿¡æ¯
            patterns = [
                r'\b(arXiv|CVPR|ICCV|ECCV|NeurIPS|ICML|ICLR|AAAI|ACL|EMNLP|NAACL)\b',
                r'\b(Proceedings of|Conference on|Workshop on)\b',
                r'\b\d{4}\b.*\b(Conference|Symposium|Workshop)\b'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    # è¿”å›ç¬¬ä¸€åŒ¹é…
                    return matches[0] if isinstance(matches[0], str) else ' '.join(matches[0])
        
        # é»˜è®¤è¿”å›
        return "Conference on AI Research"

    def _extract_authors_from_content(self, text):
        # ä¿æŒåŸæœ‰çš„ä½œè€…æå–é€»è¾‘
        clean_text = re.sub(r'[\*/a-zA-Z]*a0', '', text)
        clean_text = re.sub(r'[\*âˆ—]', '', text)

        lines = [l.strip() for l in clean_text.splitlines() if l.strip()]
        authors_line, aff_line, link = "", "", "#"

        for l in lines:
            if "university" in l.lower() or "institute" in l.lower() or "school" in l.lower() or "laboratory" in l.lower():
                aff_line += " " + l
            elif "http" in l.lower() or "https" in l.lower():
                m = re.search(r'(https?://[^\s]+)', l)
                if m:
                    link = m.group(1)
            else:
                authors_line += " " + l

        author_pattern = r'(\d+)\s*([A-Z][a-zA-Z\-\.]+\s+[A-Z][a-zA-Z\-\.]+)'
        authors_with_ids = re.findall(author_pattern, authors_line)

        if authors_with_ids:
            authors = [name.strip() for _, name in authors_with_ids]
        else:
            authors = re.findall(r'\b([A-Z][a-z]+(?:\s+[A-Z]\.)?\s+[A-Z][a-zA-Z\-]+)\b', authors_line)
            authors = [a for a in authors if len(a.split()) == 2 and not any(x.lower() in a.lower() for x in ["University", "Laboratory"])]

        aff_pattern = r'(\d+)\s+([^0-9\n]+)'
        aff_matches = re.findall(aff_pattern, aff_line)
        aff_dict = {num: aff.strip() for num, aff in aff_matches}
        if not aff_dict and aff_line:
            aff_dict = {"1": aff_line.strip()}

        affiliations = list(aff_dict.values()) or []

        return authors, affiliations, link

    def _extract_section_content(self, section_name, previously_generated=None):
        """ä½¿ç”¨æ–°çš„Agentæ ¹æ®éƒ¨åˆ†åç§°æå–å†…å®¹ï¼Œè€ƒè™‘å·²æœ‰å†…å®¹é¿å…é‡å¤"""
        # ç‰¹æ®Šå¤„ç†çš„ç« èŠ‚ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        section_name_lower = section_name.lower()
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥çš„æ¨¡å—å†…å®¹
        for module_name in self.modules_content.keys():
            if module_name.lower() == section_name_lower or section_name_lower in module_name.lower():
                content = self.modules_content[module_name].get("summary", "")
                print(f"âœ… ä»æ¨¡å—è·å–å†…å®¹: {module_name}")
                return self.formatter.format_text(content)
        
        # å¤„ç†ç‰¹æ®Šç« èŠ‚
        if section_name_lower == 'abstract':
            for sec in self.paper_content.get("sections", []):
                if "abstract" in sec.get("title", "").lower():
                    return self.formatter.format_text(sec.get("content", ""))
        
        # ä½¿ç”¨æ–°çš„Agentè¿›è¡Œåˆ†å±‚æ£€ç´¢å’Œç”Ÿæˆ
        print(f"ğŸ” ä½¿ç”¨Agentæ£€ç´¢å’Œç”Ÿæˆå†…å®¹: {section_name}")
        
        # ç¬¬ä¸€æ­¥ï¼šæ£€ç´¢ç›¸å…³æ ‡é¢˜
        paper_sections = self.paper_content.get("sections", [])
        relevant_titles = self.section_agent.retrieve_relevant_titles(section_name, paper_sections)
        
        print(f"ğŸ“š æ£€ç´¢åˆ°ç›¸å…³ç« èŠ‚: {relevant_titles}")
        
        # ç¬¬äºŒæ­¥ï¼šè·å–ç›¸å…³ç« èŠ‚çš„å†…å®¹
        relevant_sections = []
        for title in relevant_titles:
            for section in paper_sections:
                if section.get("title", "").strip() == title:
                    relevant_sections.append(section)
                    break
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå†…å®¹ï¼Œä¼ å…¥å·²ç”Ÿæˆå†…å®¹ä»¥é¿å…é‡å¤
        if relevant_sections:
            generated_content = self.section_agent.generate_section_content(
                section_name, relevant_sections, previously_generated
            )
            formatted_content = self.formatter.format_text(generated_content)
            print(f"âœ… ç”Ÿæˆå†…å®¹å®Œæˆï¼Œé•¿åº¦: {len(formatted_content)}")
            return formatted_content
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            # å°è¯•åœ¨è®ºæ–‡ç« èŠ‚ä¸­ç›´æ¥æŸ¥æ‰¾
            for sec in paper_sections:
                if section_name_lower in sec.get("title", "").lower():
                    return self.formatter.format_text(sec.get("content", ""))
            
            return f"Content for {section_name} not available."

    def _select_visuals_for_section(self, section_name, section_content):
        """ä¸ºç‰¹å®šéƒ¨åˆ†é€‰æ‹©è§†è§‰å…ƒç´ """
        candidates = []
        for fig in self.images_data.values():
            # ä½¿ç”¨åŸå§‹æ ‡é¢˜è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œå› ä¸ºåŸå§‹æ ‡é¢˜å¯èƒ½åŒ…å«æ›´å¤šä¿¡æ¯
            candidates.append({"text": fig.get("caption", ""), **fig})
        for tb in self.tables_data.values():
            text = tb.get("caption", "") + "\n" + tb.get("table_text", "")
            candidates.append({"text": text, **tb})

        if not candidates:
            return []

        top_candidates = self.bge.similarity(section_content, candidates, top_k=6)
        selected_visuals = self.qwen.decide_visuals(
            section_name, section_content, top_candidates, self.used_visuals
        )

        for v in selected_visuals:
            path = v.get("image_path") or v.get("table_path")
            self.used_visuals.add(path)

        return selected_visuals[:2]

    def _render_visual_html(self, v):
        """æ¸²æŸ“è§†è§‰å…ƒç´  - ç®€åŒ–ç‰ˆæœ¬ï¼Œè¡¨æ ¼ç›´æ¥ä½œä¸ºå›¾ç‰‡"""
        if "table_path" in v and os.path.exists(v["table_path"]):
            # è¡¨æ ¼ç›´æ¥ä½œä¸ºå›¾ç‰‡å¤„ç†
            self.table_counter += 1
            table_id = self.table_counter
            # ä½¿ç”¨æ¸…ç†åçš„æ ‡é¢˜
            caption = self.formatter.clean_caption(v.get("caption", ""))
            print(f"ğŸ“Š ç›´æ¥ä½¿ç”¨è¡¨æ ¼å›¾ç‰‡: {v.get('table_path')}")
            return self._render_table_as_image(v, caption, table_id)

        if "image_path" in v or "table_path" in v:
            relative_path = self._copy_asset(v.get("image_path") or v.get("table_path"))
            # ä½¿ç”¨æ¸…ç†åçš„æ ‡é¢˜
            caption = self.formatter.clean_caption(v.get("caption", ""))
            
            width = v.get('width', 0)
            height = v.get('height', 0)
            
            return f"""
            <div class="visual my-6 text-center">
                <div class="max-w-3xl mx-auto">
                    <img src="{relative_path}" alt="{caption}" 
                        class="w-full h-auto rounded-lg shadow-md mx-auto"
                        loading="lazy" decoding="async">
                </div>
                <p class="text-sm italic text-gray-600 mt-2">
                    {caption}
                </p>
            </div>
            """
        
        print(f"âš ï¸ ä¸æ”¯æŒçš„è§†è§‰å…ƒç´ ç±»å‹: {v.keys()}")
        return ""

    def _render_table_as_image(self, v, caption, table_id):
        """å°†è¡¨æ ¼ä½œä¸ºå›¾ç‰‡æ¸²æŸ“"""
        table_img_path = v.get("table_path")
        if table_img_path and os.path.exists(table_img_path):
            relative_path = self._copy_asset(table_img_path)
            print(f"âœ… è¡¨æ ¼ä½œä¸ºå›¾ç‰‡æ˜¾ç¤º: {table_img_path}")
            return f"""
            <div class="table-visualization my-8 bg-white rounded-xl shadow-lg overflow-hidden">
                <div class="p-6">
                    <div class="overflow-x-auto rounded-lg border border-gray-200">
                        <img src="{relative_path}" alt="{caption}" class="w-full h-auto rounded-lg">
                    </div>
                    <p class="text-sm italic text-gray-600 mt-4 text-center">
                        {caption}
                    </p>
                </div>
            </div>
            """
        else:
            return f"""
            <div class="visual my-6 bg-yellow-50 border border-yellow-200 rounded-lg p-4">
                <p class="text-sm text-yellow-800">âš ï¸ è¡¨æ ¼å›¾ç‰‡ä¸å­˜åœ¨</p>
                <p class="text-sm italic text-gray-600 mt-2 text-center">
                    {caption}
                </p>
            </div>
            """

    def _protect_citations(self, text):
        """ä¿æŠ¤å¼•ç”¨æ ‡è®°"""
        protected_text = re.sub(r'\[([^\[\]]+)\]', r'<span class="no-mathjax">[\1]</span>', text)
        return protected_text

    def _build_paper_info(self, planned_sections):
        """æ„å»ºè®ºæ–‡ä¿¡æ¯ï¼Œæ ¹æ®è§„åˆ’çš„éƒ¨åˆ†åŠ¨æ€ç”Ÿæˆå†…å®¹"""
        # æå–åŸºæœ¬ä¿¡æ¯
        basic_info = self._extract_basic_paper_info()
        title = basic_info["title"]
        authors = basic_info["authors"]
        affiliations = basic_info["affiliations"]
        links = basic_info["links"]
        
        print(f"ğŸ“Š è§„åˆ’çš„éƒ¨åˆ†: {planned_sections}")
        
        # ä¸ºæ¯ä¸ªè§„åˆ’çš„éƒ¨åˆ†ç”Ÿæˆå†…å®¹ï¼ŒæŒ‰é¡ºåºä¼ é€’å·²ç”Ÿæˆå†…å®¹
        section_contents = {}
        previously_generated = {}
        
        for section in planned_sections:
            print(f"\nğŸ“ å¤„ç†éƒ¨åˆ†: '{section}'")
            
            # ç”Ÿæˆå†…å®¹
            content = self._extract_section_content(section, previously_generated)
            protected_content = self._protect_citations(content)
            
            # å­˜å‚¨å·²ç”Ÿæˆå†…å®¹ä¾›åç»­ç« èŠ‚å‚è€ƒ
            previously_generated[section] = content
            
            # ä¸ºæœ‰å†…å®¹çš„éƒ¨åˆ†é€‰æ‹©è§†è§‰å…ƒç´ 
            visuals = []
            if content and len(content.strip()) > 50:
                visuals = self._select_visuals_for_section(section, content)
            
            # æ„å»ºè¯¥éƒ¨åˆ†çš„HTML
            section_html = f"<div class='section-content'>\n"
            section_html += f"<p>{protected_content}</p>\n"
            
            for v in visuals:
                visual_html = self._render_visual_html(v)
                section_html += visual_html + "\n"
            
            section_html += "</div>"
            section_contents[section] = section_html

        # ç”ŸæˆBibTeX
        bibkey = re.sub(r"\W+", "", title)[:15]
        bibtex = f"""@inproceedings{{{bibkey}2025,
        title={{ {title} }},
        author={{ {" and ".join(authors.split(','))} }},
        booktitle={{ {basic_info['publication_info']} }},
        year={{2025}},
        }}"""
        
        # æ„å»ºé“¾æ¥æ•°æ®
        links_data = {
            "paper": links.get("paper", "#"),
            "code": "#",  # æ¨¡æ¿ä¸­éœ€è¦çš„å ä½ç¬¦
            "dataset": "#",  # æ¨¡æ¿ä¸­éœ€è¦çš„å ä½ç¬¦
            "project_page": links.get("project_page", "#")
        }
        
        # è¿”å›å®Œæ•´ä¿¡æ¯
        paper_info = {
            "title": title,
            "authors": authors,
            "affiliations": affiliations,
            "publication_info": basic_info['publication_info'],
            "year": basic_info['year'],
            "bibtex": bibtex,
            "links": links_data,
            "planned_sections": planned_sections,
        }
        
        # æ·»åŠ å„ä¸ªéƒ¨åˆ†çš„å†…å®¹
        paper_info.update(section_contents)
        
        # æ·»åŠ æ¨¡æ¿éœ€è¦çš„å˜é‡
        paper_info["abstract"] = section_contents.get("abstract", "")
        paper_info["motivation"] = section_contents.get("motivation", "")
        paper_info["innovation"] = section_contents.get("innovation", "")
        paper_info["methodology"] = section_contents.get("methodology", "")
        paper_info["experiments"] = section_contents.get("experiments", "")
        
        return paper_info

    def _get_links_for_paper(self, paper_title):
        """æ ¹æ®è®ºæ–‡æ ‡é¢˜è·å–é“¾æ¥"""
        if paper_title in self.link_data:
            return self.link_data[paper_title]
        
        normalized_title = re.sub(r'[^\w\s]', '', paper_title).lower().strip()
        for csv_title, links in self.link_data.items():
            normalized_csv_title = re.sub(r'[^\w\s]', '', csv_title).lower().strip()
            if normalized_title == normalized_csv_title:
                return links
        
        for csv_title, links in self.link_data.items():
            paper_keywords = ' '.join(paper_title.split()[:5]).lower()
            csv_keywords = ' '.join(csv_title.split()[:5]).lower()
            
            if paper_keywords in csv_keywords or csv_keywords in paper_keywords:
                print(f"ğŸ” éƒ¨åˆ†åŒ¹é…: '{paper_title}' -> '{csv_title}'")
                return links
        
        print(f"âš ï¸ æœªåœ¨CSVä¸­æ‰¾åˆ°è®ºæ–‡é“¾æ¥: {paper_title}")
        return {'paper_url': '#', 'homepage': '#'}
    
    def _add_mathjax_support(self, html):
        """æ·»åŠ MathJaxæ”¯æŒï¼ˆå¦‚æœæ¨¡æ¿ä¸­æ²¡æœ‰ï¼‰"""
        if 'MathJax' in html:
            print("âœ… æ¨¡æ¿å·²æœ‰MathJaxæ”¯æŒ")
            return html
        
        mathjax_script = """
        <!-- MathJaxæ”¯æŒ -->
        <script>
            window.MathJax = {
                tex: {
                    inlineMath: [['$', '$']],
                    displayMath: [['$$', '$$']]
                },
                svg: {
                    fontCache: 'global'
                },
                options: {
                    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                    processEscapes: false
                }
            };
        </script>
        <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
        
        <style>
            mjx-container {
                display: inline-block !important;
                line-height: normal !important;
            }
            .MathJax {
                font-style: normal !important;
                font-weight: normal !important;
            }
            .no-mathjax {
                font-style: normal !important;
                color: inherit !important;
                background-color: transparent !important;
                display: inline !important;
            }
        </style>
        """
        
        # æ’å…¥åˆ°headæ ‡ç­¾ä¸­
        head_pattern = re.compile(r'<head>', re.IGNORECASE)
        match = head_pattern.search(html)
        
        if match:
            end_pos = match.end()
            html = html[:end_pos] + mathjax_script + html[end_pos:]
            print("âœ… æ·»åŠ MathJaxæ”¯æŒåˆ°headæ ‡ç­¾")
        else:
            # å¦‚æœæ‰¾ä¸åˆ°headæ ‡ç­¾ï¼Œåœ¨htmlæ ‡ç­¾åæ·»åŠ 
            html_pattern = re.compile(r'<html[^>]*>', re.IGNORECASE)
            html_match = html_pattern.search(html)
            if html_match:
                end_pos = html_match.end()
                html = html[:end_pos] + f'\n<head>\n{mathjax_script}\n</head>' + html[end_pos:]
                print("âœ… åˆ›å»ºheadæ ‡ç­¾å¹¶æ·»åŠ MathJaxæ”¯æŒ")
            else:
                html = f'<head>\n{mathjax_script}\n</head>\n' + html
                print("âœ… åœ¨HTMLå¼€å¤´æ·»åŠ headæ ‡ç­¾å’ŒMathJaxæ”¯æŒ")
        
        return html
    
    def _copy_static_resources_simple(self, output_folder, template_dir):
        """ç®€å•å¤åˆ¶é™æ€èµ„æºï¼Œä¸ä¿®æ”¹CSS"""
        print(f"ğŸ“ å¤åˆ¶æ¨¡æ¿èµ„æº: {template_dir} -> {output_folder}")
        
        # å¤åˆ¶æ‰€æœ‰éHTMLæ–‡ä»¶
        for item in template_dir.iterdir():
            if item.is_file() and item.suffix not in ['.html', '.jinja', '.jinja2']:
                try:
                    shutil.copy2(item, output_folder / item.name)
                    print(f"âœ… å¤åˆ¶æ–‡ä»¶: {item.name}")
                except Exception as e:
                    print(f"âŒ å¤åˆ¶æ–‡ä»¶å¤±è´¥ {item.name}: {e}")
        
        # å¤åˆ¶å­ç›®å½•ï¼ˆé™¤äº†å·²å¤„ç†çš„assetsï¼‰
        for item in template_dir.iterdir():
            if item.is_dir() and item.name != "assets" and not item.name.startswith('.'):
                dest_dir = output_folder / item.name
                try:
                    if dest_dir.exists():
                        shutil.rmtree(dest_dir)
                    shutil.copytree(item, dest_dir)
                    print(f"âœ… å¤åˆ¶ç›®å½•: {item.name}")
                except Exception as e:
                    print(f"âŒ å¤åˆ¶ç›®å½•å¤±è´¥ {item.name}: {e}")
    
    def generate_homepage(self, output_path):
        """ç”Ÿæˆä¸»é¡µ - ç®€åŒ–ç‰ˆæœ¬"""
        # è®¾ç½®è¾“å‡ºç›®å½•
        final_html_path = self._setup_output_directory(output_path)
        self.assets_mapping = {}
        
        # ç¬¬ä¸€æ­¥ï¼šæå–åŸºæœ¬ä¿¡æ¯
        print("ğŸ“‹ ç¬¬ä¸€æ­¥ï¼šæå–è®ºæ–‡åŸºæœ¬ä¿¡æ¯...")
        basic_info = self._extract_basic_paper_info()
        print(f"âœ… æå–åŸºæœ¬ä¿¡æ¯: {basic_info['title']}")
        
        # ç¬¬äºŒæ­¥ï¼šè§„åˆ’ä¸»é¡µéƒ¨åˆ†
        print("ğŸ¯ ç¬¬äºŒæ­¥ï¼šè§„åˆ’ä¸»é¡µå±•ç¤ºéƒ¨åˆ†...")
        self.planned_sections = self.planner_agent.plan_homepage_sections(
            self.paper_content, self.modules_content
        )
        print(f"âœ… è§„åˆ’å®Œæˆ: {self.planned_sections}")
        
        # ç¬¬ä¸‰æ­¥ï¼šæ„å»ºå®Œæ•´è®ºæ–‡ä¿¡æ¯
        print("ğŸ“Š ç¬¬ä¸‰æ­¥ï¼šæ„å»ºå®Œæ•´è®ºæ–‡ä¿¡æ¯...")
        paper_info = self._build_paper_info(self.planned_sections)
        
        # ç¬¬å››æ­¥ï¼šåˆå¹¶æ‰€æœ‰ä¿¡æ¯
        print("ğŸ”„ ç¬¬å››æ­¥ï¼šåˆå¹¶ä¿¡æ¯...")
        all_data = {
            **basic_info,
            **paper_info,
        }
        
        # ç¬¬äº”æ­¥ï¼šåŠ è½½åŸå§‹æ¨¡æ¿å¹¶æ¸²æŸ“
        print("ğŸ¨ ç¬¬äº”æ­¥ï¼šåŠ è½½åŸå§‹æ¨¡æ¿...")
        template_dir = Path(self.template_path).parent
        env = Environment(loader=FileSystemLoader(str(template_dir)))
        template = env.get_template(Path(self.template_path).name)
        
        print("ğŸš€ ç¬¬å…­æ­¥ï¼šæ¸²æŸ“æœ€ç»ˆHTML...")
        final_html = template.render(**all_data)
        
        # æ·»åŠ MathJaxæ”¯æŒï¼ˆå¦‚æœæ¨¡æ¿ä¸­æ²¡æœ‰ï¼‰
        if 'MathJax' not in final_html:
            print("â• æ·»åŠ MathJaxæ”¯æŒ...")
            final_html = self._add_mathjax_support(final_html)
        
        # ä¿å­˜HTMLæ–‡ä»¶
        with open(final_html_path, "w", encoding="utf-8") as f:
            f.write(final_html)
        
        # ç¬¬ä¸ƒæ­¥ï¼šå¤åˆ¶é™æ€èµ„æº
        print("ğŸ“ ç¬¬ä¸ƒæ­¥ï¼šå¤åˆ¶é™æ€èµ„æº...")
        self._copy_static_resources_simple(final_html_path.parent, template_dir)
        
        print(f"âœ… ä¸»é¡µç”Ÿæˆå®Œæˆ: {final_html_path}")
        print(f"ğŸ“ èµ„æºæ–‡ä»¶ä¿å­˜åœ¨: {self.output_assets_dir}")
        print(f"ğŸ“Š å…±å¤åˆ¶äº† {len(self.assets_mapping)} ä¸ªèµ„æºæ–‡ä»¶")
        print(f"ğŸ¯ è§„åˆ’å±•ç¤ºéƒ¨åˆ†: {self.planned_sections}")
        
        return final_html_path

# ==========================================================
# ğŸ”¹ æ‰¹é‡ç”Ÿæˆæ¨¡å¼
# ==========================================================

if __name__ == "__main__":
    # åˆå§‹åŒ–æ–‡æœ¬æ¨¡å‹
    llm_model = LLM2('Qwen2.5-7B-Instruct')
    
    template_path_list = ['/home/gaojuanru/mnt_link/gaojuanru/PaperPageAI/muban_clean/bluepaper.html']
    out_dir_list = ['orangepaper']
    
    # CSVæ–‡ä»¶è·¯å¾„
    csv_path = "/home/gaojuanru/mnt_link/gaojuanru/twittergenerate/sample_papers_updated.csv"
    
    for template_path, out_dir in zip(template_path_list, out_dir_list):
        base_dir = "/home/gaojuanru/mnt_link/gaojuanru/twittergenerate/jiexi"
        bge_path = "/mnt/gaojuanru/twittergenerate/cache/huggingface/BAAI/bge-m3"

        # éå† base_dir ä¸‹çš„æ‰€æœ‰è®ºæ–‡å­æ–‡ä»¶å¤¹
        for subdir in sorted(os.listdir(base_dir)):
            sub_path = os.path.join(base_dir, subdir)
            if not os.path.isdir(sub_path):
                continue

            # è‡ªåŠ¨åŒ¹é…å››ç±» JSON æ–‡ä»¶
            content_json = os.path.join(sub_path, f"{subdir}_content.json")
            modules_json = os.path.join(sub_path, f"{subdir}_content_modules.json")
            images_json = os.path.join(sub_path, f"{subdir}_images.json")
            tables_json = os.path.join(sub_path, f"{subdir}_tables.json")

            # ç¡®ä¿è‡³å°‘æœ‰ content å’Œ modules æ–‡ä»¶
            if not (os.path.exists(content_json) and os.path.exists(modules_json)):
                print(f"âš ï¸ ç¼ºå°‘ä¸»è¦JSONæ–‡ä»¶: {subdir}, è·³è¿‡ã€‚")
                continue

            # è¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_html = os.path.join(
                "/home/gaojuanru/mnt_link/gaojuanru/PaperPageAI/2",
                f'{out_dir}',
                f"paper_homepage_{subdir}.html"
            )
            os.makedirs(os.path.dirname(output_html), exist_ok=True)

            print(f"\nğŸš€ æ­£åœ¨ä¸º {subdir} ç”Ÿæˆä¸»é¡µ...")

            try:
                generator = PaperHomepageGenerator(
                    content_json_path=content_json,
                    modules_json_path=modules_json,
                    images_json_path=images_json if os.path.exists(images_json) else None,
                    tables_json_path=tables_json if os.path.exists(tables_json) else None,
                    template_path=template_path,
                    qwen_model=llm_model,
                    bge_path=bge_path,
                    csv_path=csv_path
                )

                generator.generate_homepage(output_html)
                print(f"âœ… {subdir} ä¸»é¡µç”ŸæˆæˆåŠŸ")
                
            except Exception as e:
                print(f"âŒ {subdir} ç”Ÿæˆå¤±è´¥: {e}")

        print("\nğŸ‰ æ‰€æœ‰å¯ç”¨è®ºæ–‡ä¸»é¡µç”Ÿæˆå®Œæˆ!")
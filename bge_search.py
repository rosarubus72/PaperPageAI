import json
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "2,3,4,5,6,7"

# ==============================
# ğŸ”¹ 1. å‘é‡æ£€ç´¢æ¨¡å— (BGE)
# ==============================
class BGEInternalRetriever:
    def __init__(self, model_path):
        """åˆå§‹åŒ– BGE æ¨¡å‹å’Œåˆ†è¯å™¨"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModel.from_pretrained(
            model_path,
            dtype=torch.float16,
            device_map="auto"
        )
        self.model.eval()
        self.sections = []  # å­˜å‚¨æ–‡æ¡£ç‰‡æ®µ
        self.embeddings = None  # å­˜å‚¨åµŒå…¥å‘é‡

    def load_document(self, json_path):
        """åŠ è½½è®ºæ–‡ JSON æ–‡ä»¶å¹¶æå–ç« èŠ‚"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for section in data.get('sections', []):
            title = section.get('title', '').strip()
            content = section.get('content', '').strip()
            if content and title not in ['References', 'Contents']:
                self.sections.append({
                    'title': title,
                    'content': content,
                    'text': f"{title}: {content}"
                })
        
        self._encode_sections()

    def _encode_sections(self):
        """æ‰¹é‡ç”ŸæˆåµŒå…¥"""
        texts = [sec['text'] for sec in self.sections]
        embeddings = []
        batch_size = 8
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            inputs = self.tokenizer(
                batch,
                padding=True,
                truncation=True,
                return_tensors='pt',
                max_length=512
            ).to(self.model.device)

            with torch.no_grad():
                output = self.model(**inputs)
                batch_emb = output.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(batch_emb)
        self.embeddings = np.vstack(embeddings)

    def _encode_query(self, query):
        """ç¼–ç æŸ¥è¯¢è¯­å¥"""
        inputs = self.tokenizer(
            query,
            padding=True,
            truncation=True,
            return_tensors='pt',
            max_length=512
        ).to(self.model.device)

        with torch.no_grad():
            output = self.model(**inputs)
            return output.last_hidden_state[:, 0, :].cpu().numpy()

    def retrieve(self, query, top_k=5):
        """æ£€ç´¢æœ€ç›¸å…³æ®µè½"""
        query_emb = self._encode_query(query)
        similarities = cosine_similarity(query_emb, self.embeddings).flatten()
        top_indices = similarities.argsort()[::-1][:top_k]

        return [
            {
                'title': self.sections[i]['title'],
                'content': self.sections[i]['content'],
                'similarity': float(similarities[i])
            }
            for i in top_indices
        ]


# ==============================
# ğŸ”¹ 2. LLM æ¨¡å— (Qwen2.5)
# ==============================
class QwenSummarizer:
    def __init__(self, model_path):
        """åˆå§‹åŒ– Qwen æ¨¡å‹"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            dtype="auto"
        )
        self.model.eval()
        print("âœ… Qwen model loaded successfully.")

    def generate_summary(self, module_name, query, retrieved_results, max_new_tokens=512):
        """æ ¹æ®æ£€ç´¢ç»“æœç”Ÿæˆæ¨¡å—æ–‡æœ¬"""
        context = ""
        for i, res in enumerate(retrieved_results, 1):
            context += f"ã€Section #{i}ã€‘\nTitle: {res['title']}\nContent: {res['content']}\n\n"

        prompt = f"""
You are an expert academic assistant helping to summarize a research paper for its online homepage.

Below are excerpts from the paper that may relate to the topic of **{module_name}**.

---
{context}
---

Now answer the following question concisely and academically:

{query}

Guidelines:
1. Base your answer only on the given excerpts.
2. Write in an academic yet accessible tone (for a project webpage).
3. Avoid unnecessary filler or repetition.
4. Keep length between 150â€“250 words.
5. Focus on clarity, coherence, and factual accuracy.
6. DO NOT generate Markdown headings (e.g., #, ##, ###) in your output.
7. You MAY use other Markdown formatting, such as bold (**text**) or italics (*text*).

Please generate the content for the "{module_name}" section directly.
"""

        messages = [
            {"role": "system", "content": "You are a scientific summarization assistant specialized in research paper interpretation."},
            {"role": "user", "content": prompt}
        ]

        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)

        generated_ids = self.model.generate(**model_inputs, max_new_tokens=max_new_tokens)
        response = self.tokenizer.batch_decode(
            generated_ids[:, model_inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )[0]

        return response.strip()


# ==============================
# ğŸ”¹ 3. ä¸»é€»è¾‘ï¼šRAG + Agent
# ==============================
def main():
    # === æ¨¡å‹è·¯å¾„ ===
    BGE_MODEL_PATH = "/home/gaojuanru/.cache/huggingface/BAAI/bge-m3"
    QWEN_MODEL_PATH = "/home/gaojuanru/.cache/huggingface/Qwen/Qwen2.5-7B-Instruct"

    # === è®ºæ–‡ JSON è·¯å¾„ ===
    JSON_PATH = "./jiexi/3D-MOOD_ Lifting 2D to 3D for Monocular Open-Set Object Detection/3D-MOOD_ Lifting 2D to 3D for Monocular Open-Set Object Detection_content.json"

    # === åˆå§‹åŒ–æ¨¡å‹ ===
    retriever = BGEInternalRetriever(BGE_MODEL_PATH)
    retriever.load_document(JSON_PATH)

    summarizer = QwenSummarizer(QWEN_MODEL_PATH)

    # === å››ä¸ªæ¨¡å—é—®é¢˜ ===
    queries = {
        "Motivation": (
            "What problem or limitation in existing methods motivates this research? "
            "Why is addressing this problem important or challenging?"
        ),
        "Innovation": (
            "What are the main innovations and key contributions of this paper? "
            "What makes the proposed approach different from previous works?"
        ),
        "Methodology": (
            "How does the proposed method work? "
            "Describe the core architecture, main modules, and the process by which it solves the problem."
        ),
        "Experiments": (
            "What experiments were conducted and what are the key findings? "
            "Summarize how the results demonstrate the effectiveness or advantages of the method."
        )
    }

    summaries = {}

    for module_name, query in queries.items():
        print(f"\n===== ğŸ§© Generating section: {module_name} =====")
        results = retriever.retrieve(query, top_k=5)

        print(f"ğŸ” Retrieved {len(results)} relevant sections for {module_name}.")
        summary = summarizer.generate_summary(module_name, query, results)

        summaries[module_name] = {
            "query": query,
            "summary": summary,
            "retrieved_sections": results
        }

        print(f"âœ… {module_name} summary generated.\n")

    # === ä¿å­˜ç»“æœ ===
    output_path = JSON_PATH.replace(".json", "_modules.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ All module summaries saved to: {output_path}")
    print("\n===== ğŸ“˜ Summary Overview =====")
    for name, data in summaries.items():
        print(f"\n### {name}\n{data['summary']}\n")


# if __name__ == "__main__":
#     main()
# ==============================
# ğŸ”¹ æ‰¹é‡å¤„ç†æ‰€æœ‰è®ºæ–‡æ–‡ä»¶å¤¹
# ==============================
def batch_generate_all():
    base_dir = "/home/gaojuanru/mnt_link/gaojuanru/PaperPageAI/jiexi"
    BGE_MODEL_PATH = "/home/gaojuanru/.cache/huggingface/BAAI/bge-m3"
    QWEN_MODEL_PATH = "/home/gaojuanru/.cache/huggingface/Qwen/Qwen2.5-7B-Instruct"

    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåŠ è½½ä¸€æ¬¡èŠ‚çœæ—¶é—´ï¼‰
    retriever = BGEInternalRetriever(BGE_MODEL_PATH)
    summarizer = QwenSummarizer(QWEN_MODEL_PATH)

    # éå†æ¯ä¸ªå­è®ºæ–‡æ–‡ä»¶å¤¹
    for paper_dir in sorted(os.listdir(base_dir)):
        paper_path = os.path.join(base_dir, paper_dir)
        if not os.path.isdir(paper_path):
            continue

        # æŸ¥æ‰¾ *_content.json æ–‡ä»¶
        content_files = [f for f in os.listdir(paper_path) if f.endswith("_content.json")]
        if not content_files:
            print(f"âš ï¸ No content.json found in {paper_dir}, skipped.")
            continue

        json_path = os.path.join(paper_path, content_files[0])
        output_path = json_path.replace("_content.json", "_content_modules.json")

        print(f"\nğŸš€ Processing paper: {paper_dir}")
        print(f"ğŸ“„ Input: {json_path}")

        try:
            # åŠ è½½è®ºæ–‡
            retriever.sections = []
            retriever.embeddings = None
            retriever.load_document(json_path)

            # å››ä¸ªæ¨¡å—é—®é¢˜
            queries = {
                "Motivation": (
                    "What problem or limitation in existing methods motivates this research? "
                    "Why is addressing this problem important or challenging?"
                ),
                "Innovation": (
                    "What are the main innovations and key contributions of this paper? "
                    "What makes the proposed approach different from previous works?"
                ),
                "Methodology": (
                    "How does the proposed method work? "
                    "Describe the core architecture, main modules, and process by which it solves the problem."
                ),
                "Experiments": (
                    "What experiments were conducted and what are the key findings? "
                    "Summarize how the results demonstrate the effectiveness or advantages of the method."
                )
            }

            summaries = {}

            for module_name, query in queries.items():
                print(f"\n===== ğŸ§© Generating section: {module_name} =====")
                results = retriever.retrieve(query, top_k=5)
                summary = summarizer.generate_summary(module_name, query, results)
                summaries[module_name] = {
                    "query": query,
                    "summary": summary,
                    "retrieved_sections": results
                }
                print(f"âœ… {module_name} done.")

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(summaries, f, ensure_ascii=False, indent=2)

            print(f"ğŸ¯ Saved to {output_path}\n")

        except Exception as e:
            print(f"âŒ Failed on {paper_dir}: {e}")

    print("\nâœ… All papers processed successfully.")


if __name__ == "__main__":
    batch_generate_all()

